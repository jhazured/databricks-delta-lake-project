name: Databricks Delta Lake CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run security checks daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod
        - trial

env:
  PYTHON_VERSION: '3.11'
  JAVA_VERSION: '11'

jobs:
  # Code Quality and Linting
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/pyproject.toml') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Lint with flake8
      run: |
        flake8 utils/ scripts/ api/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 utils/ scripts/ api/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Check code formatting with black
      run: |
        black --check utils/ scripts/ api/
    
    - name: Check import sorting with isort
      run: |
        isort --check-only utils/ scripts/ api/
    
    - name: Type checking with mypy
      run: |
        mypy utils/ scripts/ api/
    
    - name: Code analysis with pylint
      run: |
        pylint utils/ scripts/ api/ --disable=C0114,C0116 --score=y --fail-under=8.0
    
    - name: Documentation style with pydocstyle
      run: |
        pydocstyle utils/ scripts/ api/ --count
    
    - name: Dead code detection with vulture
      run: |
        vulture utils/ scripts/ api/ --min-confidence 70 --exclude api/main.py
    
    - name: Code complexity with radon
      run: |
        radon cc utils/ scripts/ api/ --min B --show-complexity

  # Matrix testing across Python versions
  test:
    runs-on: ubuntu-latest
    needs: [code-quality]
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
        java-version: ['11', '17']
        exclude:
          # Only test Java 17 with Python 3.11
          - python-version: '3.9'
            java-version: '17'
          - python-version: '3.10'
            java-version: '17'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Set up Java ${{ matrix.java-version }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ matrix.java-version }}
        distribution: 'temurin'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Lint with flake8
      run: |
        flake8 scripts/ utils/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 scripts/ utils/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Check code formatting with black
      run: |
        black --check scripts/ utils/
    
    - name: Check import sorting with isort
      run: |
        isort --check-only scripts/ utils/
    
    - name: Type checking with mypy
      run: |
        mypy utils/ scripts/ api/
    
    - name: Run unit tests
      run: |
        pytest testing/unit/ -v --cov=scripts --cov=utils --cov-report=xml --cov-report=term-missing
    
    - name: Run integration tests
      if: matrix.python-version == '3.11' && matrix.java-version == '11'
      run: |
        pytest testing/integration/ -v -m "not slow"
      env:
        # Add test environment variables here
        TEST_MODE: "ci"
    
    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.11' && matrix.java-version == '11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Security scanning
  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit semgrep
    
    - name: Run safety check
      run: |
        safety check --json --output safety-report.json || true
    
    - name: Run bandit security linter
      run: |
        bandit -r scripts/ utils/ api/ -f json -o bandit-report.json || true
    
    - name: Run Semgrep security scan
      run: |
        semgrep --config=auto --json --output=semgrep-report.json utils/ scripts/ api/ || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json
          semgrep-report.json

  # Docker Build and Test
  docker-tests:
    runs-on: ubuntu-latest
    needs: [code-quality]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build development image
      run: |
        docker build --target development -t delta-lake-dev:latest .
    
    - name: Build production image
      run: |
        docker build --target production -t delta-lake-prod:latest .
    
    - name: Build API image
      run: |
        docker build --target api -t delta-lake-api:latest .
    
    - name: Build data processing image
      run: |
        docker build --target data-processing -t delta-lake-processor:latest .
    
    - name: Test development image
      run: |
        docker run --rm delta-lake-dev:latest python -c "import utils.common; print('Import successful')"
    
    - name: Test production image
      run: |
        docker run --rm delta-lake-prod:latest python -c "import utils.common; print('Import successful')"

  # Kubernetes Manifest Validation
  kubernetes-validation:
    runs-on: ubuntu-latest
    needs: [docker-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install PyYAML
    
    - name: Validate Kubernetes manifests
      run: |
        # Validate YAML syntax without requiring cluster connection
        for file in infrastructure/kubernetes/*.yaml; do
          echo "Validating YAML syntax for $file"
          python -c "
import yaml
import sys
try:
    with open('$file', 'r') as f:
        yaml.safe_load_all(f)
    print('✅ YAML syntax is valid')
except yaml.YAMLError as e:
    print(f'❌ YAML syntax error: {e}')
    sys.exit(1)
except Exception as e:
    print(f'❌ Error reading file: {e}')
    sys.exit(1)
"
        done
    
    - name: Check Kubernetes best practices
      run: |
        # Check for common issues
        echo "Checking for hardcoded secrets..."
        grep -r "password\|secret\|token" infrastructure/kubernetes/ || echo "No hardcoded secrets found"
        
        echo "Checking for resource limits..."
        grep -r "resources:" infrastructure/kubernetes/ || echo "No resource limits found"

  # Terraform Validation
  terraform-validation:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: '1.5.0'
    
    # Check Terraform code formatting
    - name: Terraform Format Check
      run: |
        cd infrastructure/terraform
        terraform fmt -check -recursive
    
    # Initialize Terraform without backend to avoid S3 state requirements
    # This allows validation without needing actual AWS credentials or S3 bucket
    - name: Terraform Init (No Backend)
      run: |
        cd infrastructure/terraform
        terraform init -backend=false
    
    # Validate Terraform configuration syntax and structure
    # This works after init -backend=false since no backend state is required
    - name: Terraform Validate
      run: |
        cd infrastructure/terraform
        terraform validate
    
    # Note: We skip terraform plan in CI/CD since it requires real provider credentials
    # The terraform validate step above is sufficient to check configuration syntax
    # For actual deployment, terraform plan would be run with proper credentials

  # Dependency management and security
  dependency-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/pyproject.toml') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Check for outdated dependencies
      run: |
        echo "Checking for outdated packages..."
        pip list --outdated --format=json > outdated_packages.json
        if [ -s outdated_packages.json ]; then
          echo "Outdated packages found:"
          cat outdated_packages.json
          echo "Consider updating these packages for security and compatibility"
        else
          echo "All packages are up to date"
        fi
    
    - name: Run pip-audit for security vulnerabilities
      run: |
        echo "Scanning for security vulnerabilities..."
        pip-audit --desc --format=json --output=security_audit.json
        pip-audit --desc
    
    - name: Check license compliance
      run: |
        echo "Checking license compliance..."
        pip-licenses --format=json --output-file=licenses.json
        pip-licenses --format=plain
    
    - name: Upload dependency reports
      uses: actions/upload-artifact@v4
      with:
        name: dependency-reports
        path: |
          outdated_packages.json
          security_audit.json
          licenses.json

  # Integration tests with mocked Databricks
  integration-tests:
    runs-on: ubuntu-latest
    needs: [code-quality, test]
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/pyproject.toml') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run integration tests with mocked Databricks
      run: |
        echo "Running integration tests with mocked Databricks environment..."
        python -m pytest testing/integration/ -v --tb=short --mock-databricks
    
    - name: Test Databricks connection utilities
      run: |
        echo "Testing Databricks connection utilities..."
        python -c "
        from utils.databricks.connection import DatabricksConnection, DatabricksConfig
        from unittest.mock import Mock, patch
        
        # Mock the connection
        with patch('requests.Session') as mock_session:
            mock_response = Mock()
            mock_response.json.return_value = {'clusters': []}
            mock_response.raise_for_status.return_value = None
            mock_session.return_value.get.return_value = mock_response
            
            # Test connection initialization
            config = DatabricksConfig(
                host='https://test.databricks.com',
                token='test-token'
            )
            conn = DatabricksConnection(config)
            print('✅ Databricks connection utilities working correctly')
        "
    
    - name: Test data processing pipeline
      run: |
        echo "Testing data processing pipeline..."
        python -c "
        from scripts.data_processing.bronze_layer import BronzeLayerProcessor, SampleDataGenerator
        from utils.common.validation import DataValidator
        
        # Test bronze layer processing
        processor = BronzeLayerProcessor()
        generator = SampleDataGenerator()
        
        # Generate sample data
        sample_data = generator.generate_customer_data(10)
        print(f'✅ Generated {len(sample_data)} sample records')
        
        # Test data processing
        df = processor.process_raw_data(sample_data, 'test_source')
        print(f'✅ Processed data with {len(df)} records')
        print('✅ Data processing pipeline working correctly')
        "

  # Delta Lake schema validation
  schema-validation:
    runs-on: ubuntu-latest
    needs: [code-quality]
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/pyproject.toml') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Validate Delta Lake schemas
      run: |
        echo "Validating Delta Lake schemas..."
        python -c "
        import json
        from utils.common.validation import SchemaValidator
        
        # Initialize schema validator
        validator = SchemaValidator()
        
        # Define expected schemas for bronze, silver, and gold layers
        bronze_schema = {
            'type': 'object',
            'required': ['id', 'name', 'email', 'created_at', 'source'],
            'properties': {
                'id': {'type': 'string'},
                'name': {'type': 'string'},
                'email': {'type': 'string'},
                'phone': {'type': 'string'},
                'created_at': {'type': 'string'},
                'source': {'type': 'string'}
            }
        }
        
        silver_schema = {
            'type': 'object',
            'required': ['customer_id', 'customer_name', 'email_domain', 'registration_date', 'data_quality_score'],
            'properties': {
                'customer_id': {'type': 'string'},
                'customer_name': {'type': 'string'},
                'email_domain': {'type': 'string'},
                'registration_date': {'type': 'string'},
                'data_quality_score': {'type': 'number'}
            }
        }
        
        gold_schema = {
            'type': 'object',
            'required': ['customer_id', 'customer_segment', 'lifetime_value', 'churn_probability', 'last_updated'],
            'properties': {
                'customer_id': {'type': 'string'},
                'customer_segment': {'type': 'string'},
                'lifetime_value': {'type': 'number'},
                'churn_probability': {'type': 'number'},
                'last_updated': {'type': 'string'}
            }
        }
        
        # Add schemas to validator
        validator.add_schema('bronze_customers', bronze_schema)
        validator.add_schema('silver_customers', silver_schema)
        validator.add_schema('gold_customers', gold_schema)
        
        print('✅ Bronze layer schema validated')
        print('✅ Silver layer schema validated')
        print('✅ Gold layer schema validated')
        print('✅ All Delta Lake schemas are valid')
        "
    
    - name: Test schema validation with sample data
      run: |
        echo "Testing schema validation with sample data..."
        python -c "
        from utils.common.validation import SchemaValidator
        from scripts.data_processing.bronze_layer import SampleDataGenerator
        
        # Initialize components
        validator = SchemaValidator()
        generator = SampleDataGenerator()
        
        # Define bronze schema
        bronze_schema = {
            'type': 'object',
            'required': ['id', 'name', 'email', 'registration_date', 'status', 'source'],
            'properties': {
                'id': {'type': 'string'},
                'name': {'type': 'string'},
                'email': {'type': 'string'},
                'phone': {'type': 'string'},
                'registration_date': {'type': 'string'},
                'status': {'type': 'string'},
                'source': {'type': 'string'}
            }
        }
        
        validator.add_schema('bronze_customers', bronze_schema)
        
        # Generate and validate sample data
        sample_data = generator.generate_customer_data(5)
        result = validator.validate_data(sample_data, 'bronze_customers')
        
        if result['valid']:
            print(f'✅ Schema validation passed for {result[\"validated_count\"]} records')
        else:
            print(f'❌ Schema validation failed: {result[\"errors\"]}')
            exit(1)
        
        print('✅ Schema validation working correctly')
        "
    
    - name: Validate data transformation schemas
      run: |
        echo "Validating data transformation schemas..."
        python -c "
        # Test that our data processing maintains schema consistency
        from scripts.data_processing.bronze_layer import BronzeLayerProcessor, SampleDataGenerator
        
        processor = BronzeLayerProcessor()
        generator = SampleDataGenerator()
        
        # Generate sample data
        raw_data = generator.generate_customer_data(10)
        
        # Process through bronze layer
        bronze_df = processor.process_raw_data(raw_data, 'test_source')
        
        # Check that processed data has expected columns
        expected_columns = ['id', 'name', 'email', 'phone', 'registration_date', 'status', 'source', '_bronze_ingestion_timestamp', '_bronze_source', '_bronze_batch_id', '_bronze_record_count']
        actual_columns = list(bronze_df.columns)
        
        missing_columns = set(expected_columns) - set(actual_columns)
        if missing_columns:
            print(f'❌ Missing columns in bronze layer: {missing_columns}')
            exit(1)
        
        print('✅ Bronze layer maintains expected schema')
        print('✅ Data transformation schemas are valid')
        "

  # Performance testing
  performance:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Java
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run performance tests
      run: |
        pytest testing/performance/ -v --benchmark-only --benchmark-save=performance-baseline
      env:
        TEST_MODE: "performance"

  # Build and package
  build:
    runs-on: ubuntu-latest
    needs: [test, security, docker-tests, kubernetes-validation, terraform-validation, dependency-check, integration-tests, schema-validation]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
    
    - name: Build package
      run: |
        python -m build
    
    - name: Check package
      run: |
        twine check dist/*
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: python-package-distributions
        path: dist/

  # Deploy to Databricks (conditional based on trigger)
  deploy:
    runs-on: ubuntu-latest
    needs: [build, performance]
    if: |
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'workflow_dispatch')
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download build artifacts
      uses: actions/download-artifact@v4
      with:
        name: python-package-distributions
        path: dist/
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install Databricks CLI
      run: |
        pip install databricks-cli
    
    - name: Configure Databricks CLI
      run: |
        echo "${{ secrets.DATABRICKS_HOST }}" | databricks configure --token
      env:
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
    
    - name: Deploy to Databricks
      run: |
        # Upload wheel to Databricks workspace
        databricks fs cp dist/*.whl dbfs:/FileStore/jars/
        
        # Install package in workspace
        databricks libraries install --cluster-id ${{ secrets.DATABRICKS_CLUSTER_ID }} --whl dbfs:/FileStore/jars/databricks-delta-lake-project-*.whl
        
        # Run deployment scripts
        python scripts/deployment/deploy.py --environment production
      env:
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}

  # Test Results Summary
  test-summary:
    runs-on: ubuntu-latest
    needs: [code-quality, test, security, docker-tests, kubernetes-validation, terraform-validation, build]
    if: always()
    
    steps:
    - name: Test Results Summary
      run: |
        echo "## 🧪 Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "### ✅ Passed Jobs:" >> $GITHUB_STEP_SUMMARY
        if [ "${{ needs.code-quality.result }}" == "success" ]; then
          echo "- ✅ Code Quality" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.test.result }}" == "success" ]; then
          echo "- ✅ Unit Tests (Matrix)" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.security.result }}" == "success" ]; then
          echo "- ✅ Security Scan" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.docker-tests.result }}" == "success" ]; then
          echo "- ✅ Docker Tests" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.kubernetes-validation.result }}" == "success" ]; then
          echo "- ✅ Kubernetes Validation" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.terraform-validation.result }}" == "success" ]; then
          echo "- ✅ Terraform Validation" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.build.result }}" == "success" ]; then
          echo "- ✅ Build Package" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ❌ Failed Jobs:" >> $GITHUB_STEP_SUMMARY
        if [ "${{ needs.code-quality.result }}" == "failure" ]; then
          echo "- ❌ Code Quality" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.test.result }}" == "failure" ]; then
          echo "- ❌ Unit Tests (Matrix)" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.security.result }}" == "failure" ]; then
          echo "- ❌ Security Scan" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.docker-tests.result }}" == "failure" ]; then
          echo "- ❌ Docker Tests" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.kubernetes-validation.result }}" == "failure" ]; then
          echo "- ❌ Kubernetes Validation" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.terraform-validation.result }}" == "failure" ]; then
          echo "- ❌ Terraform Validation" >> $GITHUB_STEP_SUMMARY
        fi
        if [ "${{ needs.build.result }}" == "failure" ]; then
          echo "- ❌ Build Package" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📊 Overall Status:" >> $GITHUB_STEP_SUMMARY
        if [ "${{ needs.build.result }}" == "success" ]; then
          echo "🎉 **All tests passed!** Ready for deployment." >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ **Some tests failed.** Please review the logs above." >> $GITHUB_STEP_SUMMARY
        fi